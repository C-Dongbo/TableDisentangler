<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Biomed Eng Online</journal-id><journal-title>BioMedical Engineering OnLine</journal-title><issn pub-type="epub">1475-925X</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">16219098</article-id><article-id pub-id-type="pmc">1266388</article-id><article-id pub-id-type="publisher-id">1475-925X-4-58</article-id><article-id pub-id-type="doi">10.1186/1475-925X-4-58</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>A coarse-to-fine approach to prostate boundary segmentation in ultrasound images</article-title></title-group><contrib-group><contrib id="A1" contrib-type="author"><name><surname>Sahba</surname><given-names>Farhang</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I2">2</xref><email>fsahba@uwaterloo.ca</email></contrib><contrib id="A2" corresp="yes" contrib-type="author"><name><surname>Tizhoosh</surname><given-names>Hamid R</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I2">2</xref><email>tizhoosh@uwaterloo.ca</email></contrib><contrib id="A3" contrib-type="author"><name><surname>Salama</surname><given-names>Magdy M</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I3">3</xref><email>msalama@hivolt.uwaterloo.ca</email></contrib></contrib-group><aff id="I1"><label>1</label>Medical Instrument Analysis and Machine Intelligence Group, University of Waterloo, Waterloo, Canada</aff><aff id="I2"><label>2</label>Department of Systems Design Engineering, 200 University Avenue West, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada</aff><aff id="I3"><label>3</label>Department of Electrical and Computer Engineering, 200 University Avenue West, University of Waterloo, Waterloo, Ontario, N2L 3G1, Canada</aff><pub-date pub-type="collection"><year>2005</year></pub-date><pub-date pub-type="epub"><day>11</day><month>10</month><year>2005</year></pub-date><volume>4</volume><fpage>58</fpage><lpage>58</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedical-engineering-online.com/content/4/1/58"/><history><date date-type="received"><day>4</day><month>8</month><year>2005</year></date><date date-type="accepted"><day>11</day><month>10</month><year>2005</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2005 Sahba et al; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2005</copyright-year><copyright-holder>Sahba et al; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>
               Sahba
               Farhang
               
               
               fsahba@uwaterloo.ca
            </dc:author><dc:title>
            A coarse-to-fine approach to prostate boundary segmentation in ultrasound images
         </dc:title><dc:date>2005</dc:date><dcterms:bibliographicCitation>BioMedical Engineering OnLine 4(1): 58-. (2005)</dcterms:bibliographicCitation><dc:identifier type="sici">1475-925X(2005)4:1&#x0003c;58&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1475-925X</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p>In this paper a novel method for prostate segmentation in transrectal ultrasound images is presented.</p></sec><sec sec-type="methods"><title>Methods</title><p>A segmentation procedure consisting of four main stages is proposed. In the first stage, a locally adaptive contrast enhancement method is used to generate a well-contrasted image. In the second stage, this enhanced image is thresholded to extract an area containing the prostate (or large portions of it). Morphological operators are then applied to obtain a point inside of this area. Afterwards, a Kalman estimator is employed to distinguish the boundary from irrelevant parts (usually caused by shadow) and generate a coarsely segmented version of the prostate. In the third stage, dilation and erosion operators are applied to extract outer and inner boundaries from the coarsely estimated version. Consequently, fuzzy membership functions describing regional and gray-level information are employed to selectively enhance the contrast within the prostate region. In the last stage, the prostate boundary is extracted using strong edges obtained from selectively enhanced image and information from the vicinity of the coarse estimation.</p></sec><sec><title>Results</title><p>A total average similarity of 98.76%(&#x000b1; 0.68) with gold standards was achieved.</p></sec><sec><title>Conclusion</title><p>The proposed approach represents a robust and accurate approach to prostate segmentation.</p></sec></abstract></article-meta></front><body><sec><title>1 Introduction</title><p>Prostate cancer is one of the most frequently diagnosed forms of cancer in the male population and the second cancer-related cause of death for this group [<xref ref-type="bibr" rid="B1">1</xref>,<xref ref-type="bibr" rid="B2">2</xref>]. Ultrasound imaging is a widely used technology for prostate biopsy. The accurate detection of the prostate boundary in ultrasound images is crucial for some clinical applications, such as the accurate placement of the needles during the biopsy, accurate prostate volume measurement from multiple frames, constructing anatomical models used in treatment planning and estimation of tumor border. These images are the result of reflection, refraction and deflection of ultrasound beams from different types of tissues with different acoustic impedance [<xref ref-type="bibr" rid="B3">3</xref>]. However, in ultrasound images the contrast is usually low and the boundaries between the prostate and background are fuzzy. Also speckle and weak edges make the ultrasound images inherently difficult to segment. Furthermore, the quality of the image depends on the type and particular settings of the machine. All these factors make the analysis of ultrasound images challenging. But it still remains an important image modality for clinical applications and an automatic segmentation of these images is highly desirable. This work is organized as follows: In section II existing literature on prostate segmentation is briefly reviewed. The motivation for this work is summarized at the end of section II. Section III introduces the proposed approach and describes its main components in detail. Section IV validates the performance of the method via visual inspection and some quantitative measures. Section V concludes the work.</p></sec><sec><title>2 Related work</title><p>Currently, the prostate boundaries are generally extracted from TRUS images [<xref ref-type="bibr" rid="B3">3</xref>]. As previously mentioned, in TRUS images of the prostate, the signal-to-noise ratio is very low. Therefore, traditional edge detectors fail to extract the correct boundaries. Consequently, many methods have been introduced to facilitate more accurate and automatic or semi-automatic segmentation of the prostate boundaries from the ultrasound images.</p><p>Knoll <italic>et al</italic>. [<xref ref-type="bibr" rid="B4">4</xref>] considered deformable contours for prostate segmentation in medical images for both initialization and modeling. They have proposed a method based on a one-dimensional dyadic wavelet transform as a multiscale contour parameterization technique to constrain the shape of the prostate model.</p><p>Richard <italic>et al</italic>. [<xref ref-type="bibr" rid="B5">5</xref>] presented an algorithm which segments a set of parallel 2D images of the prostate into prostate and non-prostate regions to form a 3D image of the prostate. This texture-based algorithm is a pixel classifier based on four texture energy measures associated with each pixel in the image. Clustering techniques are then used to label each pixel in the image with the label of the most probable class.</p><p>Arnink <italic>et al</italic>. [<xref ref-type="bibr" rid="B6">6</xref>] described a method for determination of the contour of the prostate in ultrasound images. They have used an edge detection technique based on nonlinear Laplace filtering. The method then combines the information about edge location and strength to construct an edge intensity image. Finally, edges representing a boundary are selected and linked to build the final outline.</p><p>Ladak <italic>et al</italic>. [<xref ref-type="bibr" rid="B7">7</xref>,<xref ref-type="bibr" rid="B19">19</xref>] proposed an algorithm for semi-automatic segmentation of the prostate from 2D ultrasound images. The algorithm uses model-based initialization and a discrete dynamic contour. First, the user must select four points around the prostate. Then the outline of the prostate is estimated using cubic interpolation functions and shape information. Finally, the estimated contour is deformed automatically to better fit the image. This semi-automatic algorithm can segment a wide range of prostate images, but at least four initial points must be selected manually by the user (radiologist).</p><p>Prater <italic>et al</italic>. [<xref ref-type="bibr" rid="B8">8</xref>] presented a method for segmentation of the prostate in transrectal ultrasound images based on feed-forward neural networks. This method segments images to prostate and non-prostate regions. Three neural network architectures have been proposed. These networks are trained using a small portion of a training image segmented by an expert and then applied to the entire training image.</p><p>Wang <italic>et al</italic>. [<xref ref-type="bibr" rid="B30">30</xref>] presented two methods for semiautomatic three-dimensional (3-D) prostate boundary segmentation using 2-D ultrasound images. The segmentation process is initiated by manually placing four points on the boundary of a selected slice. Then an initial prostate boundary is determined. It is refined using the Discrete Dynamic Contour until it fit the actual prostate boundary. The remaining slices are then segmented by iteratively propagating the result to another slices and implementing the refinement.</p><p>Hu <italic>et al</italic>. [<xref ref-type="bibr" rid="B31">31</xref>] proposed an algorithm for semiautomatic segmentation of the prostate from 3D ultrasound images. In this method the authors use model-based initialization and mesh refinement using deformable models. Six points are required to initialize the outline of the prostate using shape information. The initial outline is then automatically deformed to better fit the prostate boundary.</p><p>Chiu emphet al. [<xref ref-type="bibr" rid="B32">32</xref>] introduced a semi-automatic segmentation algorithm based on the dyadic wavelet transform and the discrete dynamic contours. In this method first a spline interpolation is used to determine the initial contour based on four user-defined initial points. Then the discrete dynamic contour refines the initial contour based on the approximate coefficients and the wavelet coefficients generated using the dyadic wavelet transform. A selection rule is used as well to choose the best contour based.</p><p>Abolmaesumi <italic>et al</italic>. [<xref ref-type="bibr" rid="B33">33</xref>] used a segmentation technique to extract prostate contours from Transrectal Ultrasound images. In this method an Sticks filter is used to reduce the speckle. The problem is then discretized by projecting equispaced radii from an arbitrary seed point inside the prostate cavity towards its boundary. Candidate edge points obtained along each radius include the measurement points and some false returns. This modelling approach is used for prostate contour extraction.</p><p>Ghanei <italic>et al</italic>. [<xref ref-type="bibr" rid="B9">9</xref>] proposed a three-dimensional deformable surface model for prostate segmentation based on a discrete structure which is made from a set of vertices in the 3D space as triangle facets. The model converges from using a weighted sum of the internal and external forces. The model is initialized manually from a few human-drawn polygons drawn on different slices.</p><p>Pathak <italic>et al</italic>. [<xref ref-type="bibr" rid="B10">10</xref>] proposed an algorithm for guided edge delineation which provides automatic prostate edge detection as a visual guide to the observer. It is followed by manual editing. The edge detection algorithm contains three stages. First, the sticks algorithm is used to enhance contrast and reduce speckle in the image. Second, the resulting image is smoothed using an anisotropic diffusion filter. Finally, some basic prior knowledge of the prostate, such as shape and echo pattern, is used to detect the most probable edges which indicate the prostate shape. In the last stage, the information is integrated by using a manual linking procedure on the detected edges.</p><p>Shen <italic>et al</italic>. [<xref ref-type="bibr" rid="B11">11</xref>] introduced a statistical shape model to segment the prostate in transrectal ultrasound images. First, a Gabor filter bank is used in both multiple scales and multiple orientations to characterize the prostate boundaries. The Gabor features are reconstructed to be invariant to the rotation of the ultrasound probe. Then, a hierarchical deformation strategy is used. The model focuses on the similarity of different Gabor features at different deformation stages using a multiresolution technique. The authors have also introduced an adaptive focus deformable model, which uses the concept of an attribute vector [<xref ref-type="bibr" rid="B12">12</xref>].</p><p>In another work Gong <italic>et al</italic>. [<xref ref-type="bibr" rid="B28">28</xref>] presented an approach based on deformable models. In this technique, model initialization and constraining model evolution are based on prior knowledge about the prostate shape. The prostate shape has been modeled using deformable superellipses.</p><p>Betrouni <italic>et al</italic>. [<xref ref-type="bibr" rid="B29">29</xref>] discussed a method for the automatic segmentation of trans-abdominal ultrasound images of the prostate. In this method a filter is used to enhance the contours without changing the information in the image. Adaptive morphological and median filtering are employed to detect the noise-containing regions and smooth these areas. Then a heuristic optimization algorithm begins to search for the contour initialized from a prostate model.</p><p>Generally, prostate segmentation methods have limitations when the image contains shadows with similar gray level and texture attached to the prostate, and/or missing boundary segments. In these cases the segmentation error may increase considerably. Another obstacle may be the lack of a sufficient number of training (gold) samples if a learning technique is employed. Algorithms based on active contours have been quite successfully implemented with the major drawback that they depend on user interaction to determine the seed points (initial snake).</p><p>Based on an analysis of the existing literature a new approach should ideally possess certain properties:</p><p>&#x02022; User interaction (e.g. defining seed points) may not be always desirable due to its drawbacks such as time consumption, human error etc. The new technique, therefore, should require a minimum level of user interaction.</p><p>&#x02022; Providing a large number of training samples in medical environments is generally difficult, specially if the samples are being prepared by an expert. Hence, sample-based learning should be avoided.</p><p>&#x02022; The approach must be robust with respect to the presence of noise and shadow.</p><p>In this paper, by introducing a multi-stage, coarse-to-fine approach, we establish a straightforward algorithm which attempts to satisfy the above conditions as much as possible. In this method some input parameters must be adjusted for series of images (i.e. images captured with a certain equipment setting).</p></sec><sec><title>3 Proposed approach</title><p>In this section, we will present a new region- and grayscale-based approach to prostate segmentation in ultrasound images. The proposed approach contains four main stages (see Fig. <xref ref-type="fig" rid="F1">1</xref>):</p><fig position="float" id="F1"><label>Figure 1</label><caption><p>Schematic diagram of the proposed approach.</p></caption><graphic xlink:href="1475-925X-4-58-1"/></fig><p>&#x02022; <bold>Pre-Processing: </bold>After smoothing, using a locally-adaptive contrast enhancement technique, a primary version of the image which has sufficient contrast across the image is produced (see section 3.1).</p><p>&#x02022; <bold>Coarse Segmentation: </bold>The obtained image is thresholded and some morphological operators are applied until an isolated object related to the prostate is produced. The central point of this object can be considered as the central point of the prostate. Subsequently, using a Kalman estimator, a coarse estimation for the prostate boundary is obtained. This coarse estimation is not accurate, but we just use it as input for the next stage (see section 3.2).</p><p>&#x02022; <bold>Selective Enhancement: </bold>Using coarse estimation, one eroded and one dilated version can be produced. These are used to establish regional fuzzy membership functions. By defining a fuzzy inference system based on the produced membership functions, a selective contrast enhancement can be obtained in the area of the prostate (see section 3.3).</p><p>&#x02022; <bold>Prostate Segmentation: </bold>Finally, the algorithm finds potential boundary pieces and extracts the prostate boundary (see section 3.4).</p><p>In the following sections, each stage of the proposed approach will be described in detail.</p><sec><title>3.1 Pre-processing</title><p>Transrectal ultrasound images are heavily corrupted with noise. Since we just need a rough estimation for the next stage, we can remove the noise using a median filter (7 &#x000d7; 7 or 9 &#x000d7; 9). It should be mentioned that this smoothed version is only used for the next step. For the final segmentation we use the "original" ultrasound image. Therefore, the median filter does not manipulate the edge information needed for the final segmentation. Fig. <xref ref-type="fig" rid="F2">2</xref> shows the original TRUS and its smoothed version.</p><fig position="float" id="F2"><label>Figure 2</label><caption><p>Top: Original image, Bottom: Smoothed image using a median filter 9 &#x000d7; 9.</p></caption><graphic xlink:href="1475-925X-4-58-2"/></fig><p>In order to enhance the contrast of the image, a locally-adaptive method based on fuzzy sets has been used to create a primary version which has enough contrast [<xref ref-type="bibr" rid="B13">13</xref>,<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B25">25</xref>].</p><p>In a fuzzy approach, an image can be presented as an array of fuzzy singletons (a membership function with only one supporting point). Each of these singletons has a value that denotes the membership of each pixel to a predefined image property. The properties can be defined globally for the whole image or locally for sub-images [<xref ref-type="bibr" rid="B13">13</xref>]. In the case of contrast enhancement using fuzzy techniques, some parameters of spatial domain, such as minimum gray-level <italic>g</italic><sub><italic>min </italic></sub>or maximum gray-level <italic>g</italic><sub><italic>max</italic></sub>, are needed. In a global method, finding those parameters is simple, but if we want to enhance the image in a locally approach, then we must calculate these parameters for each local neighborhood to adjust the membership function. Tizhoosh <italic>et al</italic>. [<xref ref-type="bibr" rid="B13">13</xref>] have presented a locally adaptive approach to find the parameters for some support points for an <italic>M </italic>&#x000d7; <italic>N</italic>-image and interpolate these values to obtain corresponding values for each pixel. It is clear that these interpolated parameters are not precise, but because the concept of fuzziness is incorporated, the input data have not to be exact [<xref ref-type="bibr" rid="B13">13</xref>,<xref ref-type="bibr" rid="B14">14</xref>]. In the method proposed in [<xref ref-type="bibr" rid="B13">13</xref>], first the image is divided into <italic>M</italic><sub><italic>S </italic></sub>rows and <italic>N</italic><sub><italic>S</italic></sub>. This leads to a matrix with the size of <italic>M</italic><sub><italic>S </italic></sub>&#x000d7; <italic>N</italic><sub><italic>S</italic></sub>. Each element in this matrix corresponds to a supporting point for the image. A window around each supporting point is considered to find the local information for that point. In a general case the size of window around each supporting point depends on the degree of homogeneity or edginess of local pixels in the corresponding sub-image. Small window sizes do not only increase the computational cost but also sometimes fail to capture correct information due to noise sensitivity. On the other hand, with large window sizes, values obtained by interpolated parameters lose their accuracy. In our application we use constant window sizes around each supporting point to find the local information. Just to avoid loss of information during the interpolation, the size of windows is chosen in a way that they have enough overlap with each other. Using the local windows we can extract local parameters, like <italic>g</italic><sub><italic>min</italic>,<italic>local </italic></sub>and <italic>g</italic><sub><italic>max</italic>,<italic>local</italic></sub>. Applying a 2-D interpolation function (e.g. linear or cubic), the membership function parameters can be calculated for the whole image [<xref ref-type="bibr" rid="B13">13</xref>].</p><p>To apply this method in our case we have used a maximum window size of 40 &#x000d7; 40 and a minimum window size of 20 &#x000d7; 20, both determined heuristically. The algorithm calculates a proper window size between these two values using fuzzy rules. Among methods introduced in [<xref ref-type="bibr" rid="B13">13</xref>] we have employed a fuzzy rule-based approach to enhance the contrast for the entire image [<xref ref-type="bibr" rid="B13">13</xref>-<xref ref-type="bibr" rid="B16">16</xref>]. The input membership functions used in this fuzzy rule-based approach are shown in Fig. <xref ref-type="fig" rid="F3">3</xref>.</p><fig position="float" id="F3"><label>Figure 3</label><caption><p>Input membership function for locally adaptive contrast enhancement. It represents three terms for the fuzzy set "pixel intensity". Generally <italic>b </italic>is in the middle value of LocalMin and LocalMax, <italic>a </italic>is the middle values of LocalMin and <italic>b</italic>, and <italic>c </italic>is the middle value of LocalMax and <italic>b</italic>.</p></caption><graphic xlink:href="1475-925X-4-58-3"/></fig><p>For the output membership function we have used three singleton values <italic>g<sub>S1 </sub></italic>= 10, <italic>g<sub>S2 </sub></italic>= 100 and <italic>g<sub>S3 </sub></italic>= 255.</p><p>The result of applying the above method on the smoothed image followed by global thresholding on the enhanced image are shown in Fig. <xref ref-type="fig" rid="F4">4</xref> and <xref ref-type="fig" rid="F5">5</xref>, respectively. As we can see in Fig. <xref ref-type="fig" rid="F4">4</xref>, applying the primary contrast enhancement makes a significant gray level difference between the dark areas, including prostate, and bright gaps around the prostate.</p><fig position="float" id="F4"><label>Figure 4</label><caption><p>Primary high contrast version of the smoothed image from Fig. 2.</p></caption><graphic xlink:href="1475-925X-4-58-4"/></fig><fig position="float" id="F5"><label>Figure 5</label><caption><p>(a): The result of global thresholding on primary high contrast version in Fig. 4, (b): Filled version of the global thresholded image.</p></caption><graphic xlink:href="1475-925X-4-58-5"/></fig><p>For global thresholding, all gray levels <italic>g </italic>less than a threshold value <italic>T </italic>are set to zero and those greater than this threshold are set to 255 such that a binary image with black and white values <italic>g' </italic>is obtained. Fig. <xref ref-type="fig" rid="F5">5(a)</xref> shows the result of applying global thresholding to the image in Fig. <xref ref-type="fig" rid="F4">4</xref>. The main advantage of applying the primary contrast enhancement is that the bright gaps produced around the prostate have distinctly large gray level intensities. Consequently, an image with a bimodal histogram is produced. <italic>T </italic>is chosen as a middle value between two peaks in the histogram and for a large category of ultrasound images, good performance of global thresholding can be expected.</p></sec><sec><title>3.2 Coarse estimation</title><p>This section demonstrates how a central point inside the prostate can be found. We will see that using this point and some other information how a Kalman estimator achieves a coarse estimation of the prostate boundary.</p><p>The binary image obtained after thresholding contains several black holes (Fig <xref ref-type="fig" rid="F5">5(a)</xref>). These holes can be filled such that a completely white region is archived (Fig <xref ref-type="fig" rid="F5">5(b)</xref>) [<xref ref-type="bibr" rid="B17">17</xref>]. Applying the primary contrast enhancement stage ensures the extraction of a white area corresponding to the prostate in the thresholded image. This area must be isolated from other white regions. One straightforward method uses the morphological opening [<xref ref-type="bibr" rid="B17">17</xref>]. This operator must be applied with a large structuring element to isolate the object corresponding to the prostate. Considering the geometrical location and the measurement of area, the isolated object with high likelihood to be part of the prostate can be distinguished. A point <italic>O </italic>inside the prostate is required to perform the next stage. This point can be simply obtained by finding the centroid point of this isolated object.</p><p>Now we want to find a coarse estimation of the prostate boundary. This can be solved using a Kalman filter [<xref ref-type="bibr" rid="B18">18</xref>] for tracing the edges. We can use some properties of this filter to extract the valid data from the thresholded image and remove the irrelevant parts. To implement such a system we interpret the problem of edge tracing as a dynamic tracking system. In this system the pixels located on the edge of an object of interest are used as the input (measurement data) for the tracking filter. Using such a system the Kalman filter can track a trajectory along the border of the object. Each new pixel along the border brings updated information for the current and future position. We assume each pixel along the border to be in a dynamic movement. For this movement we can consider the position and velocity as the variables which describe the state of the system. Using a Kalman filter we can estimate the position which is actually the border of the object of interest. In fact we use a tracking system to follow the border. In a 2D image we have two variables for position and two variables for velocity. Because of the shape of the prostate it is more efficient to represent the state variables in the form of relative polar coordinates. We can use the internal point <italic>O </italic>to define such state variables and consider the following state vector:</p><p><inline-graphic xlink:href="1475-925X-4-58-i1.gif"/></p><p>where <italic>r </italic>is the distance between the internal point <italic>O</italic>(<italic>x</italic><sub><italic>c</italic></sub>, <italic>y</italic><sub><italic>c</italic></sub>) and pixels (<italic>x</italic><sub><italic>p</italic></sub>, <italic>y</italic><sub><italic>p</italic></sub>) located on the border of the prostate and <italic>&#x003b8; </italic>is the angle between the vertical axis and <inline-graphic xlink:href="1475-925X-4-58-i2.gif"/>. Hence, the following equations can be considered for r, <italic>&#x003b8;</italic>, <inline-graphic xlink:href="1475-925X-4-58-i3.gif"/> and <inline-graphic xlink:href="1475-925X-4-58-i4.gif"/>:</p><p><inline-graphic xlink:href="1475-925X-4-58-i5.gif"/></p><p><inline-graphic xlink:href="1475-925X-4-58-i6.gif"/></p><p><inline-graphic xlink:href="1475-925X-4-58-i7.gif"/></p><p><inline-graphic xlink:href="1475-925X-4-58-i8.gif"/></p><p>where <inline-graphic xlink:href="1475-925X-4-58-i3.gif"/>, <inline-graphic xlink:href="1475-925X-4-58-i4.gif"/> are the radial and angular velocity, respectively. Using the above state vector we represent the sequential pixels on the border of the object of interest (corresponding to the movement of vehicle on a path) by radial and angular position and velocity. Kalman filter considers a discrete dynamic model contains state and measurement equations:</p><p><bold>x</bold><sub><italic>k </italic></sub>= <bold>A</bold><sub><italic>k</italic>-1</sub><bold>x</bold><sub><italic>k</italic>-1 </sub>+ <bold>B</bold><sub><italic>k</italic>-1</sub><bold>W</bold><sub><italic>k</italic>-1</sub>, &#x000a0;&#x000a0;&#x000a0; (6)</p><p><bold>Z</bold><sub><italic>k </italic></sub>= <bold>H</bold><sub><italic>k</italic></sub><bold>x</bold><sub><italic>k </italic></sub>+ <bold>V</bold><sub><italic>k</italic></sub>, &#x000a0;&#x000a0;&#x000a0; (7)</p><p>where <bold>x </bold>is the state vector in equation 1. Other components are defined as follows:</p><p><inline-graphic xlink:href="1475-925X-4-58-i9.gif"/></p><p><inline-graphic xlink:href="1475-925X-4-58-i10.gif"/></p><p><inline-graphic xlink:href="1475-925X-4-58-i11.gif"/></p><p>where <bold>W</bold><sub><italic>k </italic></sub>~ <italic>N</italic>(0,<italic>Q</italic><sub><italic>k</italic></sub>) and <bold>V</bold><sub><italic>k </italic></sub>~ <italic>N</italic>(0, <italic>R</italic><sub><italic>k</italic></sub>) are the process and measurement noise, respectively. <italic>T </italic>is the interval which represents the changes in the state and measurement equations. The value of <italic>T </italic>does not affect the final result and therefore we can choose it as <italic>T </italic>= l for simplicity. <italic>R </italic>and <italic>Q </italic>are diagonal matrices. The values in these matrixes are the measurement and process noise covariance, respectively. In the area that border is changing smoothly the measurement data is placed inside the association gate and the value of measurement noise <italic>R </italic>should be small. In the case when there is no data inside the association gate we cannot be ensure about the validity of measurement data. Therefore the value of measurement noise <italic>R </italic>should be large. The large and small values for the measurement noise depend on the quality of image and can be adjusted between 5 to 100. The value of process noise in matrix <italic>Q </italic>simulates the small variation around the estimated point. For process noise we can consider a value between 4 to 8. In the discrete dynamic equations the accelerations in the radius and angle (the changes of radial and angular velocities) is modeled as a zero-mean, white, Gaussian noise <bold>W</bold>. Also the measurement data <bold>Z</bold><sub><italic>k </italic></sub>which is the location of boundary pixels in the form of <italic>r </italic>and <italic>&#x003b8; </italic>is assumed to be a noisy version of the actual position of the boundary. Kalman filtering is done using prediction and update steps. The prediction equations include</p><p><bold>x</bold><sub><italic>k</italic>|<italic>k</italic>-1 </sub>= <bold>A</bold><sub><italic>k</italic>-1</sub><bold>x</bold><sub><italic>k</italic>-1|<italic>k</italic>-1</sub>, &#x000a0;&#x000a0;&#x000a0; (11)</p><p><inline-graphic xlink:href="1475-925X-4-58-i12.gif"/></p><p>whereas update equations can be given as follows</p><p><inline-graphic xlink:href="1475-925X-4-58-i13.gif"/></p><p><bold>K</bold><sub><italic>k</italic>-1 </sub>= <bold>P</bold><sub><italic>k</italic>|<italic>k</italic>-1</sub><bold>H</bold><sub><italic>k</italic>-1</sub><bold>S</bold><sup>-1</sup>, &#x000a0;&#x000a0;&#x000a0; (14)</p><p><bold>x</bold><sub><italic>k</italic>-1|<italic>k</italic>-1 </sub>= <bold>x</bold><sub><italic>k</italic>-1|<italic>k</italic>-2 </sub>+ <bold>K</bold><sub><italic>k</italic>-1</sub>[<bold>Z</bold><sub><italic>k</italic>-1 </sub>- <bold>H</bold><sub><italic>k</italic>-1</sub><bold>x</bold><sub><italic>k</italic>-1|<italic>k</italic>-2</sub>], &#x000a0;&#x000a0;&#x000a0; (15)</p><p>where <bold>P </bold>is error covariance matrix, <bold>K </bold>is kalman gain and <bold>S </bold>is innovation covariance matrix. For calculation of the coarse version, the Kalman filter receives its initial data from the nearest point placed on the vertical axis and on the top of the internal point <italic>O</italic>. Then it starts the estimation using the data located on the border of the object in the thresholded image. In each sequential iteration, the points along the prostate border can be used as measured data and the Kalman filter estimates the next <italic>r </italic>and <italic>&#x003b8;</italic>. These predicted values determine a pixel as the next pixel on the border. Also it predicts <inline-graphic xlink:href="1475-925X-4-58-i3.gif"/> and <inline-graphic xlink:href="1475-925X-4-58-i4.gif"/> for the next iteration. When we go to the next iteration the new pixel on the border is the new measured data for the filter. This data is compared to the predicted position from the previous iteration. If there is sufficient correlation between them the measured data is incorporated to update the filter state, otherwise the prediction point is considered as measured point and after updating the filter starts the next iteration. To measure the correlation we implement an association process between the predicted and measured data. For this association process we use a gate, the so-called "association gate" around the predicted pixel. Only the pixels located on the border and inside of this gate are considered as valid measured data for updating the filter. For good performance, the association gate must be adaptive. This means that the size of this gate must be varied based on the covariance of the Kalman filter. The gate must maximize the presence of valid data and minimize invalid data. To find such a gate, the probability ratio function, <italic>L</italic>, at the <italic>k</italic><sup><italic>th </italic></sup>iteration must be maximized [<xref ref-type="bibr" rid="B18">18</xref>]:</p><p><inline-graphic xlink:href="1475-925X-4-58-i14.gif"/></p><p>where <italic>p</italic><sub>1</sub>(<italic>k</italic>) and <italic>p</italic><sub>0</sub>(<italic>k</italic>) are the probabilities of the presence of valid data and invalid data in the gate, respectively. This prompts us to check the value of a statistical distance <italic>d </italic>with respect to a threshold <italic>D </italic>as follows [<xref ref-type="bibr" rid="B18">18</xref>]:</p><p><italic>d</italic><sup>2 </sup>&#x02264; <italic>D</italic>, &#x000a0;&#x000a0;&#x000a0; (17)</p><p>with</p><p><italic>d</italic><sup>2 </sup>= <bold>l</bold><sup>T</sup><bold>P</bold><sup>-1</sup><bold>l</bold>, &#x000a0;&#x000a0;&#x000a0; (18)</p><p>where <bold>l </bold>is the vector of geometrical distance between the predicted point and an arbitrary point <italic>l </italic>= (<italic>l</italic>1, <italic>l</italic>2), and <bold>P </bold>is the covariance matrix of the Kalman filter. In two dimensional problems, equation 18 is reduced to the following form:</p><p><inline-graphic xlink:href="1475-925X-4-58-i15.gif"/></p><p>where <italic>a, b, c </italic>and <italic>e </italic>are the values of covariance matrix. This is an equation for the points inside of an ellipse. The value of <italic>D </italic>is generally a constant between 1 to 10. In the case when there is no data inside the association gate we need to capture the data. Therefore, the value of <italic>D </italic>should gradually increase. Fig. <xref ref-type="fig" rid="F6">6</xref> illustrates this local polar coordinate schema and elliptical gating.</p><fig position="float" id="F6"><label>Figure 6</label><caption><p>local polar coordinate schema and elliptical gating for extracting the coarse estimation.</p></caption><graphic xlink:href="1475-925X-4-58-6"/></fig><p>When the border reaches a shadow, or a missing boundary segment is encountered, then there is an abrupt change in the pixel path. These sharp changes are considered as new paths for tracking process. After a few iterations Kalman filter detects that such cases do not belong to the true path because the data do not correlate with the followed path. The filter adaptively enlarges the association gate until it again captures the true data on the border which have enough correlation with the prediction. Using the association technique the data belonging to shadows and missing segments on the prostate border are detected and eliminated. During the process, Kalman estimator follows the prostate border variations in a coarse manner and irrelevant parts are isolated. Fig. <xref ref-type="fig" rid="F7">7</xref> shows this coarse estimation for the prostate.</p><fig position="float" id="F7"><label>Figure 7</label><caption><p>Isolated object corresponding to the prostate and its center.</p></caption><graphic xlink:href="1475-925X-4-58-7"/></fig><p>In the most parts, the gray level difference produced by primary contrast enhancement makes the prostate border distinguishable after thresholding. Applying the Kalman estimator ensures that the prostate is isolated from any irrelevant parts and a coarse estimation is achieved. An interesting result is that if the quality of ultrasound images are good enough there is no irrelevant parts attached to the prostate and the boundary can be visible after primary contrast enhancement and thresholding. In these cases we do not need to employ the Kalman filter and coarse estimation may even be used as the final segmented image.</p><p>The quality of this coarse estimation directly depends on the quality of the ultrasound image in terms of original contrast, the presence and intensity of shadow, the noise and so on. For all tested images the extracted coarse estimation had sufficient accuracy to be passed to subsequent stages.</p></sec><sec><title>3.3 Selective enhancement</title><p>In this section a new regional approach is introduced to increase the prostate contrast. This will amplify the strength of the outer prostate edges.</p><p>The previous section contained some techniques to find a coarse estimation. Using the boundary generated by the coarse estimation, two contours can be obtained such that the true boundary is ideally located between them. In our approach, these are called inner and outer contours. For extracting the inner contour, an erosion operator with a disk-shaped structuring element can be employed on the coarse estimation. With the same structuring element and using a dilation operator, the outer contour can be obtained as well. These two contours are employed as points of departure to define a membership function.</p><p>This membership function determines to what degree does a pixel belong to the prostate. Fig. <xref ref-type="fig" rid="F8">8</xref> illustrates the membership function <italic>&#x003bc;</italic><sub><italic>location </italic></sub>based on pixel position. Because the boundary of the coarse estimation is extracted from the object corresponding to the prostate, it can be assumed that the true edges of the prostate are located around this boundary. If the pixel is located outside of the outer contour, the membership value is 0, if it is inside of the inner contour, the membership value is 1. For the pixels in between, the nearest distance from the inner and outer contours is determined and the value of membership is calculated. Because the pixels located between the inside contour and the coarse estimation boundary most likely belong to the prostate, the membership function has more emphasis on this interval.</p><fig position="float" id="F8"><label>Figure 8</label><caption><p>the membership function <italic>&#x003bc;</italic><sub><italic>location </italic></sub>based on pixel position. In this Figure <inline-graphic xlink:href="1475-925X-4-58-i16.gif"/> and <italic>d</italic><sub><italic>inside</italic></sub>and <italic>d</italic><sub><italic>outside </italic></sub>are the shortest euclidian distance to the outside and inside contours respectively.</p></caption><graphic xlink:href="1475-925X-4-58-8"/></fig><p>Similarly, this function has less emphasis for the pixels located between the coarse estimation boundary and the outer contour because the pixels located around the outer contour most likely do not belong to the prostate. In fact, the membership value <italic>&#x003bc; </italic>depends on the following equation:</p><p><inline-graphic xlink:href="1475-925X-4-58-i17.gif"/></p><p>where <italic>&#x003b1; </italic>= 6, <italic>&#x003b2; </italic>= 0.5 (heuristically selected parameters for the sigmoidal function) and the value of <italic>d </italic>is calculated as follows</p><p><inline-graphic xlink:href="1475-925X-4-58-i18.gif"/></p><p>where <italic>d</italic><sub><italic>inside </italic></sub>and <italic>d</italic><sub><italic>outside </italic></sub>are the shortest euclidian distance of arbitrary points to the outer and inner contours, respectively. For the establishment of a fuzzy inference system, we need to define appropriate fuzzy rules [<xref ref-type="bibr" rid="B13">13</xref>,<xref ref-type="bibr" rid="B14">14</xref>]. The area between the two contours is important for the extraction of the prostate boundary. Since we assume that there is generally a dark to light transition from the inside to outside of the prostate, any algorithm improving the strength of edges in this area is useful. A straightforward method is to make the dark and gray pixels darker and bright pixels brighter. This can increase the strength of the edges around the prostate boundary. In addition, as previously discussed, we want to enhance the contrast just for the area within the prostate. Therefore, simple rules can be defined as follows:</p><p>&#x02022; <bold>IF </bold>the pixel <italic>does not belong </italic>to the prostate, <bold>THEN </bold>leave it <italic>unchanged</italic></p><p>&#x02022; <bold>IF </bold>the pixel <italic>belongs </italic>to the prostate <bold>AND </bold>is <italic>dark</italic>, <bold>THEN </bold>make it <italic>darker</italic></p><p>&#x02022; <bold>IF </bold>the pixel <italic>belongs </italic>to the prostate <bold>AND </bold>is <italic>gray</italic>, <bold>THEN </bold>make it <italic>dark</italic></p><p>&#x02022; <bold>IF </bold>the pixel <italic>belongs </italic>to the prostate <bold>AND </bold>is <italic>bright</italic>, <bold>THEN </bold>make it <italic>brighter</italic></p><p>The last rule is mainly designed to enhance the brighter boundary pixels (bright pixels within the prostate are not relevant at this stage). The membership function for input gray levels is shown in Fig. <xref ref-type="fig" rid="F9">9</xref>. In this figure <italic>T</italic><sub>4 </sub>is the brightest gray level in the image. <italic>T</italic><sub>1</sub>, <italic>T</italic><sub>2 </sub>and <italic>T</italic><sub>3 </sub>can be calculated based on local information, but for simplicity we have used the values 25, 50 and 80, respectively. For the output membership function, fuzzy singletons with the values <italic>G</italic><sub><italic>S</italic>1</sub><italic>= </italic>1, <italic>G</italic><sub><italic>S</italic>2 </sub>= 64 and <italic>G</italic><sub><italic>S</italic>3</sub><italic>= </italic>255 are defined empirically.</p><fig position="float" id="F9"><label>Figure 9</label><caption><p>Membership functions for input gray level values.</p></caption><graphic xlink:href="1475-925X-4-58-9"/></fig><p>Using these rules we enhance the contrast of the image not only based on gray level values but also based on the location of each pixel. The result is shown in Fig. <xref ref-type="fig" rid="F10">10</xref>. It clearly demonstrates that the result is an image with higher contrast in the prostate area.</p><fig position="float" id="F10"><label>Figure 10</label><caption><p>Top: Original image. Bottom: Enhanced Image using proposed fuzzy inference.</p></caption><graphic xlink:href="1475-925X-4-58-10"/></fig></sec></sec><sec><title>3.4 Final segmentation</title><p>In the previous section we achieved an image with a highly contrasted prostate area and almost no changes in other regions such that strong edges were created in the boundaries of the prostate. Also we know that the true boundary is located between the inner and outer contours and somewhere most likely close to the coarse estimation. Therefore, we must first detect the edges located between these two contours. For this purpose, a Canny edge detector is used [<xref ref-type="bibr" rid="B21">21</xref>]. Fig. <xref ref-type="fig" rid="F11">11</xref> shows the result for the area between two contours. This image contains many potential boundary pieces. Among these pieces, those with higher likelihood of being a true edge should be considered. We start from a point above the internal point <italic>O</italic>, traverse along the coarse boundary and consider small pieces (3 to 5 pixels) on this boundary, sequentially. For each edge segment on the coarse boundary there are several edge pieces. We use three criteria to extract the final edges:</p><fig position="float" id="F11"><label>Figure 11</label><caption><p>The result of applying Canny edge detection on the enhanced image (Fig. 10). Solid line is the coarse estimation outline.</p></caption><graphic xlink:href="1475-925X-4-58-11"/></fig><p>1. The average distance <inline-graphic xlink:href="1475-925X-4-58-i19.gif"/> of <italic>N </italic>candidate pixels in the vicinity of the coarse boundary &#x02013; we can calculate the vicinity distance by the following equation:</p><p><inline-graphic xlink:href="1475-925X-4-58-i20.gif"/></p><p>where <italic>d</italic><sub><italic>i </italic></sub>is the distance of <italic>i</italic><sup><italic>th </italic></sup>pixel with respect to the boundary of the coarse estimation.</p><p>2. The value of the gradient &#x02013; In the preceding section, we enhanced the contrast of the prostate area. This increases the local gradient values of the pixels located on the true boundary pieces. The local gradient value in 3 &#x000d7; 3 neighborhoods is determined as:</p><p><inline-graphic xlink:href="1475-925X-4-58-i21.gif"/></p><p>where <italic>G</italic><sub><italic>x </italic></sub>and <italic>G</italic><sub><italic>y </italic></sub>are the gradient values in <italic>x </italic>and <italic>y </italic>directions, respectively.</p><p>3. The angle of edge pieces with respect to the coarse boundary &#x02013; we can calculate the absolute value of angle between the coarse boundary piece and edge piece. This value can vary from 0 to <italic>&#x003c0;</italic>/2. Zero radians reflects the most and <italic>&#x003c0;</italic>/2 the least compatibility.</p><p>After these criteria are considered in the presented priority order, the final boundaries must be extracted from potential pieces. Using the above criteria we must extract the edge pieces which have the greatest likelihood of being a true edge. Among all pieces, a piece with minimum distance, maximum gradient, or minimum angle with respect to the boundary of the coarse estimation should be chosen. If we consider the border of the coarse version where there was no shadow and missing boundary segment, the information of the coarse version is more reliable. Therefore, in these parts our criteria for choosing an edge piece is the distance of that piece with respect to the coarse version. If the distances are equal, then the angles of pieces are considered. If the angles are equal as well, then the gradient values are compared. But in the parts where the Kalman filter has estimated the border of the coarse estimation the strengths of the edge pieces (using gradient values) are the only criteria for the selection. Subsequently, the algorithm goes directly to the next piece and continues the procedure until the complete prostate outline is achieved. For the coarse boundary pieces, if there is no edge piece around them we can continue the edge extracted in the previous coarse boundary piece so that it has the minimum angle with respect to the coarse estimation. A straightforward way to fill the gaps between adjacent pieces is to fill them by straight lines. Finally, we can again apply a Kalman filter to smooth these edges and make a consistent outline for the prostate to achieve the final result. This smoothing filter is just like what we used to find the coarse estimation except that it uses all final data on the prostate border.</p><p>A sample result of employing the proposed approach on the extracted edge is shown in Fig. <xref ref-type="fig" rid="F12">12</xref>. This is the final result for the prostate segmentation. This figure shows the outline obtained manually by a radiologist (solid line). Visually, the difference between the two contours is negligible. To evaluate the algorithm for a low quality case, we have applied the proposed method in the image shown in Fig. <xref ref-type="fig" rid="F13">13</xref>. As we can see, the segmentation in the strongly shadowed areas have some error but in the other areas, the result is almost the same as manual segmentation.</p><fig position="float" id="F12"><label>Figure 12</label><caption><p>Automatic and manual boundaries are shown on the original images. Solid lines are manually segmented images and dash lines are the result of proposed algorithm.</p></caption><graphic xlink:href="1475-925X-4-58-12"/></fig><fig position="float" id="F13"><label>Figure 13</label><caption><p>A low quality TRUS image and the result of automatic and manual boundaries shown on the original images. Solid lines are manually segmented images and dash lines are the result of proposed algorithm.</p></caption><graphic xlink:href="1475-925X-4-58-13"/></fig></sec><sec><title>4 Experiments and results</title><p>Other algorithms mentioned in section 2 have used different numbers of ultrasound images (from 8 [<xref ref-type="bibr" rid="B11">11</xref>] to 90 [<xref ref-type="bibr" rid="B7">7</xref>,<xref ref-type="bibr" rid="B19">19</xref>]) to validate their approaches. They have compared the algorithm-based segmentations with the manual segmentations (as a gold standard). We have selected the images to include regular and difficult cases and examined 42 different TRUS images. The images were noisy, with low contrast and shadow effects. The results of the proposed method have been evaluated by comparing the algorithm-based segmentation and the manual segmentation (gold standards).</p><p>As it was mentioned throughout the paper some parameters need to be adjusted in this algorithm. But for a set of similar TRUS images (i.e. images captured with a certain machine setting), most of them do not require any change. In the conducted experiments, the following parameter configurations have been used:</p><p>&#x02022; The size of the median filter used for smoothing was 7 &#x000d7; 7.</p><p>&#x02022; The structuring element used in the opening procedure was a disk with diameter 15.</p><p>For quantitative evaluation, we have used the following error measures to validate the performance of our segmentation compared to the manual segmentation by a radiologist for those images [<xref ref-type="bibr" rid="B22">22</xref>]:</p><p><bold>Distance </bold><italic>&#x003b4; </italic>= Average Euclidean distance (in pixels) between the algorithm-based segmentation and the manual segmentation. For each pixel the distance is defined as the shortest Euclidean distance between that pixel and the pixels located on the other contour.</p><p><bold>Area </bold><italic>E</italic><sub><italic>A </italic></sub>= Area error (%) = 100.<inline-graphic xlink:href="1475-925X-4-58-i22.gif"/>,</p><p>where <italic>S</italic><sub><italic>Man </italic></sub>is the area of the manual segmentation and <italic>S</italic><sub><italic>Alg </italic></sub>is the area of the algorithm-based segmentation.</p><p><bold>Similarity </bold>In addition, we have used a <italic>similarity measure</italic>, <italic>&#x003b7;</italic>, based on the misclassification rate as a more general criterion in image segmentation [<xref ref-type="bibr" rid="B23">23</xref>,<xref ref-type="bibr" rid="B24">24</xref>]:</p><p><inline-graphic xlink:href="1475-925X-4-58-i23.gif"/></p><p>where <italic>B</italic><sub><italic>0 </italic></sub>and <italic>F</italic><sub><italic>0 </italic></sub>denote the background and foreground of the original image (manually segmented), <italic>B</italic><sub><italic>T </italic></sub>and <italic>F</italic><sub><italic>T </italic></sub>denote the background and foreground area pixel in the result image, and |.| is the set cardinality.</p><p>Table <xref ref-type="table" rid="T1">I</xref> summarizes the results along with the average, <italic>m</italic>, and standard deviation, <italic>&#x003c3;</italic>, for the three quantitative measures for all 42 test images.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Quantitative evaluation of proposed approach in comparison with manual segmentation (gold standard). The average <italic>m </italic>and deviation <italic>&#x003c3; </italic>for three performance measures are calculated for 42 test images and their corresponding gold standards.</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Number</td><td align="left"><italic>&#x003b4;</italic></td><td align="left"><italic>E</italic><sub><italic>A</italic></sub></td><td align="left"><italic>&#x003b7;</italic></td></tr></thead><tbody><tr><td align="left"><italic>m</italic></td><td align="left"><bold>3.67</bold></td><td align="left"><bold>5.62</bold></td><td align="left"><bold>98.76</bold></td></tr><tr><td align="left"><italic>&#x003c3;</italic></td><td align="left"><bold>1.08</bold></td><td align="left"><bold>2.98</bold></td><td align="left"><bold>0.68</bold></td></tr></tbody></table></table-wrap><p>Considering the quality of the images in terms of the shadow effect, the quantitative results are promising. From the results, it can be seen that in the areas that there is no shadow the proposed method is able to deliver very accurate results. In the areas, in which strong shadows cover the prostate the result of the algorithm is still acceptable. Of course, in these areas error is increased because the gray level and the texture of the shadow is very similar to those of the prostate. Qualitatively, the difference between the algorithm-based segmentations and the manual segmentations is not considerable. The proposed approach, implemented in MatLab, needs (in average) less than 10<italic>s </italic>to segment the prostate using a 2.8 <italic>GHz </italic>Pentium IV. However, this time measurement is based on experimental setup. It can be expected that a considerable speedup can still be achieved if the algorithm is implemented and optimized in realtime platforms such as C++.</p></sec><sec><title>5 Conclusion</title><p>A novel approach to prostate segmentation containing a coarse estimation and a new selective fuzzy contrast enhancement model has been presented in this paper. Because of the characteristics of the ultrasound images, we first smooth the original image using two filters. This smoothed image is enhanced using a locally-adaptive contrast technique. The output image has large gaps with high intensity around the prostate. Using global thresholding and morphological operators, an isolated object containing the prostate was obtained. The center of this object was considered as an internal point of the prostate. A Kalman estimator with the polar coordinates was implemented to find a coarse estimation of the prostate border. Using erosion and dilation of this estimation, inner and outer contours were obtained. A fuzzy inference system based on these regional contours and spatial gray level information was designed to selectively enhance the contrast of original image within the prostate region. The output of this fuzzy enhancement system provided an image with high contrast and strong edges on the prostate borders. Finally, the edges between inner and outer contours were extracted. In order to correctly recognize the prostate boundaries, potential boundary pieces were marked and based on pixels gradients, the vicinity and angle relative to the coarse estimation boundary, the final segmentation was achieved. The proposed approach has been examined for typical TRUS images. In comparison with manually segmented images, the experimental results show that our approach can segment the prostate boundary accurately. A total average similarity of 98.76%(&#x000b1; 0.68) with the gold standards was achieved. The test images contain not only regular but also noisy, low-contrasted and shadowy cases. In this approach we have designed a straightforward and fast algorithm with minimum level of user interaction. It also does not need training samples for implementation. More samples containing the manual prostate segmentation by radiologists are desired in order to verify the segmentation accuracy more reliably. In addition, by using an adaptive approach for noise reduction, we can improve the image quality, especially the prostate edges. This can give us better results in subsequent stages. In addition, developing of a similar technique for 3D prostate segmentation can be a subject for further work.</p></sec><sec><title>6 Authors' contributions</title><p>Farhang Sahba developed the main parts of the approach. Hamid R. Tizhoosh provided guidance and was involved in the theoretical aspects of algorithm development. Magdy M. Salama provided guidance. All authors read and approved the final manuscript.</p></sec></body><back><ref-list><ref id="B1"><citation citation-type="other"><person-group person-group-type="author"><collab>American Cancer Society</collab></person-group><article-title>Cancer Facts and Figures</article-title><year>2002</year><ext-link ext-link-type="uri" xlink:href="http://www.cancer.org"/></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mettlin</surname><given-names>C</given-names></name></person-group><article-title>American society national cancer detection project</article-title><source>Cancer</source><year>1995</year><volume>75</volume><fpage>1790</fpage><lpage>1794</lpage></citation></ref><ref id="B3"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Insana</surname><given-names>MF</given-names></name><name><surname>Brown</surname><given-names>DG</given-names></name></person-group><article-title>Acoustic scattering theory applied to soft biological tissues</article-title><source>Ultrasonic Scattering in biological tissues</source><year>1993</year><publisher-name>CRC Press, Boca Raton</publisher-name><fpage>76</fpage><lpage>124</lpage></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Knoll</surname><given-names>C</given-names></name><name><surname>Alcaniz</surname><given-names>M</given-names></name><name><surname>Grau</surname><given-names>V</given-names></name><name><surname>Monserrat</surname><given-names>C</given-names></name><name><surname>Juan</surname><given-names>MC</given-names></name></person-group><article-title>Outlining of the prostate using snakes with shape restrictions based on the wavelet transform</article-title><source>Pattern Recognition</source><year>1999</year><volume>32</volume><fpage>1767</fpage><lpage>1781</lpage><pub-id pub-id-type="doi">10.1016/S0031-3203(98)00177-0</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Richard</surname><given-names>WD</given-names></name><name><surname>Keen</surname><given-names>CG</given-names></name></person-group><article-title>Automated texture-based segmentation of ultrasound images of the prostate</article-title><source>Computerized Medical Imaging and Graphics</source><year>1996</year><volume>20</volume><fpage>131</fpage><lpage>140</lpage><pub-id pub-id-type="pmid">8930465</pub-id><pub-id pub-id-type="doi">10.1016/0895-6111(96)00048-1</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Aarnink</surname><given-names>RG</given-names></name><name><surname>Giesen</surname><given-names>RJB</given-names></name><name><surname>Huynen</surname><given-names>AL</given-names></name><name><surname>Rosette</surname><given-names>JJM</given-names></name><name><surname>Debruyne</surname><given-names>FMJ</given-names></name><name><surname>Wijkstra</surname><given-names>H</given-names></name></person-group><article-title>A practical clinical method for contour determination in ultrasonographic prostate images</article-title><source>Ultrasound in Medicine and Biology</source><year>1994</year><volume>20</volume><fpage>705</fpage><lpage>717</lpage><pub-id pub-id-type="pmid">7863560</pub-id><pub-id pub-id-type="doi">10.1016/0301-5629(94)90028-0</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ladak</surname><given-names>HM</given-names></name><name><surname>Mao</surname><given-names>F</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Downey</surname><given-names>DB</given-names></name><name><surname>Steinman</surname><given-names>DA</given-names></name><name><surname>Fenster</surname><given-names>A</given-names></name></person-group><article-title>Prostate boundary segmentation from 2D ultrasound images</article-title><source>Medical Physics</source><year>2000</year><volume>27</volume><fpage>1777</fpage><lpage>1788</lpage><pub-id pub-id-type="pmid">10984224</pub-id><pub-id pub-id-type="doi">10.1118/1.1286722</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Prater</surname><given-names>JS</given-names></name><name><surname>Richard</surname><given-names>WD</given-names></name></person-group><article-title>Segmenting ultrasound images of the prostrate using neural networks</article-title><source>Ultrasound Imaging</source><year>1992</year><volume>14</volume><fpage>159</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/0161-7346(92)90005-G</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ghanei</surname><given-names>A</given-names></name><name><surname>Soltanian-Zadeh</surname><given-names>H</given-names></name><name><surname>Ratkesicz</surname><given-names>A</given-names></name><name><surname>Yin</surname><given-names>F</given-names></name></person-group><article-title>A three-dimensional deformable model for segmentation of human prostate from ultrasound image</article-title><source>Medical Physics</source><year>2001</year><volume>28</volume><fpage>2147</fpage><lpage>2153</lpage><pub-id pub-id-type="pmid">11695777</pub-id><pub-id pub-id-type="doi">10.1118/1.1388221</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pathak</surname><given-names>SD</given-names></name><name><surname>Chalana</surname><given-names>V</given-names></name><name><surname>Haynor</surname><given-names>DR</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name></person-group><article-title>Edge-guided boundary delineation in prostate ultrasound images</article-title><source>IEEE Transactions on Medical Imaging</source><year>2000</year><volume>19</volume><fpage>1211</fpage><lpage>1219</lpage><pub-id pub-id-type="pmid">11212369</pub-id><pub-id pub-id-type="doi">10.1109/42.897813</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>D</given-names></name><name><surname>Zhan</surname><given-names>Y</given-names></name><name><surname>Davatzikos</surname><given-names>C</given-names></name></person-group><article-title>Segmentation of Prostate Boundaries From Ultrasound Images Using Statistical Shape Model</article-title><source>IEEE Transactions on Medical Imaging</source><year>2003</year><volume>22</volume><fpage>539</fpage><lpage>551</lpage><pub-id pub-id-type="pmid">12774900</pub-id><pub-id pub-id-type="doi">10.1109/TMI.2003.809057</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>D</given-names></name><name><surname>Herskovits</surname><given-names>EH</given-names></name><name><surname>Davatzikos</surname><given-names>C</given-names></name></person-group><article-title>An adaptive-focus statistical shape model for segmentation and shape modeling of 3D brain structures</article-title><source>IEEE Transactions on Medical Imaging</source><year>2001</year><volume>20</volume><fpage>257</fpage><lpage>270</lpage><pub-id pub-id-type="pmid">11370893</pub-id><pub-id pub-id-type="doi">10.1109/42.921475</pub-id></citation></ref><ref id="B13"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Tizhoosh</surname><given-names>HR</given-names></name><name><surname>Krell</surname><given-names>G</given-names></name><name><surname>Muchaelis</surname><given-names>B</given-names></name></person-group><article-title>Locally adaptive fuzzy image enhancement</article-title><source>Computational intelligence, Theory and applications</source><year>1997</year><publisher-name>Springer, Germany</publisher-name><fpage>272</fpage><lpage>276</lpage></citation></ref><ref id="B14"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Pal</surname><given-names>SK</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Zadeh LA, Yager RR</surname></name></person-group><article-title>Fuzziness, Image Information and Scene analysis</article-title><source>An introduction to fuzzy Logic Application in Intelligent Systems</source><year>1996</year><publisher-name>Kluwer Academic Publishers, USA</publisher-name></citation></ref><ref id="B15"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zadeh</surname><given-names>LA</given-names></name></person-group><article-title>A Fuzzy-set-theoretic Interpretation of Linguistic Hedges</article-title><source>Journal of Cybernetics</source><year>1972</year><volume>2</volume><fpage>4</fpage><lpage>34</lpage></citation></ref><ref id="B16"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Tizhoosh</surname><given-names>HR</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Kerre E, Nachtegael M</surname></name></person-group><article-title>Fuzzy Image Enhancement: An Overview</article-title><source>Fuzzy Techniques in Image Processing</source><year>2000</year><publisher-name>Springer, Studies in Fuzziness and Soft Computing</publisher-name><fpage>137</fpage><lpage>171</lpage></citation></ref><ref id="B17"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Goutsias</surname><given-names>J</given-names></name><name><surname>Vincent</surname><given-names>L</given-names></name><name><surname>Bloomberg</surname><given-names>DS</given-names></name></person-group><source>Mathematical Morphology and its Application to Image and Signal Processing</source><year>2000</year><publisher-name>Kluwer Academic Publisher</publisher-name></citation></ref><ref id="B18"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Gelb</surname><given-names>A</given-names></name></person-group><source>Applied Optimal Estimation</source><year>1974</year><publisher-name>MIT Press</publisher-name></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Fenster</surname><given-names>A</given-names></name><name><surname>Downey</surname><given-names>D</given-names></name></person-group><article-title>Three-dimensional ultrasound imaging</article-title><source>Medical Physics</source><year>1999</year><volume>3659</volume><fpage>2</fpage><lpage>11</lpage></citation></ref><ref id="B20"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Kerre</surname><given-names>EE</given-names></name><name><surname>Nachtegael</surname><given-names>M</given-names></name></person-group><article-title>Fuzzy Techniques in Image Processing</article-title><source>Physica-Verlag</source><year>2000</year><publisher-name>Springer, Germany</publisher-name></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Canny</surname><given-names>J</given-names></name></person-group><article-title>A Computational Approach to Edge Detection</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>1986</year><volume>8</volume><fpage>679</fpage><lpage>698</lpage></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gamio</surname><given-names>JC</given-names></name><name><surname>Belongie</surname><given-names>SJ</given-names></name><name><surname>Majumdar</surname><given-names>S</given-names></name></person-group><article-title>Normalized Cuts in 3-D for Spinal MRI Segmentation</article-title><source>IEEE Transactions on Medical Imaging</source><year>2004</year><volume>23</volume><fpage>36</fpage><lpage>44</lpage><pub-id pub-id-type="pmid">14719685</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sankur</surname><given-names>B</given-names></name><name><surname>Sezgin</surname><given-names>M</given-names></name></person-group><article-title>Survey over image thresholding technique and quantitative performance evaluation</article-title><source>Journal of Electronic Imaging</source><year>2004</year><volume>13</volume><fpage>146</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1117/1.1631315</pub-id></citation></ref><ref id="B24"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yasnoff</surname><given-names>WA</given-names></name><name><surname>Mui</surname><given-names>JK</given-names></name><name><surname>Bacus</surname><given-names>JW</given-names></name></person-group><article-title>Error measures for scene segmentation</article-title><source>Pattern Recognition</source><year>1977</year><volume>9</volume><fpage>217</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1016/0031-3203(77)90006-1</pub-id></citation></ref><ref id="B25"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Sahba</surname><given-names>F</given-names></name><name><surname>Tizhoosh</surname><given-names>HR</given-names></name><name><surname>Salama</surname><given-names>MMA</given-names></name></person-group><article-title>A New technique for Adaptive fuzzy image enhancement for edge detection</article-title><source>International workshop on multidisciplinary image, video and audio retrieval and mining</source><year>2004</year><publisher-name>Sherbrooke, Canada</publisher-name></citation></ref><ref id="B26"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Grimm</surname><given-names>PD</given-names></name><name><surname>Balsko</surname><given-names>JC</given-names></name><name><surname>Ragde</surname><given-names>H</given-names></name></person-group><article-title>Ultrasound guided transperineal implantation of iodine 125 and palladium 103 for the treatment of early stage prostate cancer</article-title><source>Atlas of the Urologic Clinics of North America</source><year>1994</year><volume>2</volume><fpage>113</fpage><lpage>125</lpage></citation></ref><ref id="B27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Terris</surname><given-names>MK</given-names></name><name><surname>Stamey</surname><given-names>TA</given-names></name></person-group><article-title>Determination of prostate volume by transrectal ultrasound</article-title><source>Journal of Urology</source><year>1991</year><volume>145</volume><fpage>984</fpage><lpage>987</lpage><pub-id pub-id-type="pmid">2016815</pub-id></citation></ref><ref id="B28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>L</given-names></name><name><surname>Pathak</surname><given-names>SD</given-names></name><name><surname>Haynor</surname><given-names>DR</given-names></name><name><surname>Cho</surname><given-names>PS</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name></person-group><article-title>Parametric Shape Modeling Using Deformable Superellipses for Prostate Segmentation</article-title><source>IEEE Transactions on Medical Imaging</source><year>2004</year><volume>23</volume><fpage>340</fpage><lpage>349</lpage><pub-id pub-id-type="pmid">15027527</pub-id><pub-id pub-id-type="doi">10.1109/TMI.2004.824237</pub-id></citation></ref><ref id="B29"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Betrounia</surname><given-names>N</given-names></name><name><surname>Vermandela</surname><given-names>M</given-names></name><name><surname>Pasquierc</surname><given-names>D</given-names></name><name><surname>Maoucheb</surname><given-names>S</given-names></name><name><surname>Rousseaua</surname><given-names>J</given-names></name></person-group><article-title>Segmentation of abdominal ultrasound images of the prostate using a priori information and an adapted noise filter</article-title><source>Computerized Medical Imaging and Graphics</source><year>2005</year><volume>29</volume><fpage>43</fpage><lpage>51</lpage><pub-id pub-id-type="pmid">15710540</pub-id><pub-id pub-id-type="doi">10.1016/j.compmedimag.2004.07.007</pub-id></citation></ref><ref id="B30"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Cardinal</surname><given-names>HN</given-names></name><name><surname>Downey</surname><given-names>DB</given-names></name><name><surname>Fenster</surname><given-names>A</given-names></name></person-group><article-title>Semiautomatic three-dimensional segmentation of the prostate using two-dimensional ultrasound images</article-title><source>Medical physics</source><year>2003</year><volume>30</volume><fpage>887</fpage><lpage>897</lpage><pub-id pub-id-type="pmid">12772997</pub-id><pub-id pub-id-type="doi">10.1118/1.1568975</pub-id></citation></ref><ref id="B31"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>N</given-names></name><name><surname>Downey</surname><given-names>DB</given-names></name><name><surname>Fenster</surname><given-names>A</given-names></name><name><surname>Ladak</surname><given-names>HM</given-names></name></person-group><article-title>Prostate boundary segmentation from 3D ultrasound images</article-title><source>Medical Physics</source><year>2003</year><volume>30</volume><fpage>1648</fpage><lpage>1659</lpage><pub-id pub-id-type="pmid">12906182</pub-id><pub-id pub-id-type="doi">10.1118/1.1586267</pub-id></citation></ref><ref id="B32"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chiu</surname><given-names>B</given-names></name><name><surname>Freeman</surname><given-names>GH</given-names></name><name><surname>Salama</surname><given-names>MMA</given-names></name><name><surname>Fenster</surname><given-names>A</given-names></name></person-group><article-title>Prostate segmentation algorithm using dyadic wavelet transform and discrete dynamic contour</article-title><source>Phys Med Biol</source><year>2004</year><volume>49</volume><fpage>4943</fpage><lpage>4960</lpage><pub-id pub-id-type="pmid">15584529</pub-id><pub-id pub-id-type="doi">10.1088/0031-9155/49/21/007</pub-id></citation></ref><ref id="B33"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Abolmaesumi</surname><given-names>P</given-names></name><name><surname>Sirouspour</surname><given-names>MR</given-names></name></person-group><article-title>Segmentation of prostate contours from ultrasound images</article-title><source>ICASSP04</source><year>2004</year><volume>3</volume><fpage>517</fpage><lpage>520</lpage></citation></ref></ref-list></back></article>


